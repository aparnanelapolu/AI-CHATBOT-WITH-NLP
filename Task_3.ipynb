{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJ17chu24bJK",
        "outputId": "a1bc41bb-9549-4988-871a-08a54690dfd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chatbot: Hello! How can I assist you today?\n",
            "You: hello\n",
            "Chatbot: Hi there!\n",
            "You: what is your name\n",
            "Chatbot: I am an AI assistant created to chat with you.\n",
            "You: ok tell me about computers new version\n",
            "Chatbot: I am a chatbot created to help you!\n",
            "You: ok\n",
            "Chatbot: Greetings!\n",
            "You: thankyou\n",
            "Chatbot: Greetings!\n",
            "You: exit\n",
            "Chatbot: Goodbye!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import random\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "intents = {\n",
        "    \"intents\": [\n",
        "        {\n",
        "            \"tag\": \"greeting\",\n",
        "            \"patterns\": [\"Hi\", \"Hello\", \"How are you?\", \"Hey\", \"Howdy\"],\n",
        "            \"responses\": [\"Hello!\", \"Hi there!\", \"Hey!\", \"Greetings!\"]\n",
        "        },\n",
        "        {\n",
        "            \"tag\": \"goodbye\",\n",
        "            \"patterns\": [\"Bye\", \"Goodbye\", \"See you\", \"Later\"],\n",
        "            \"responses\": [\"Goodbye!\", \"See you later!\", \"Bye!\", \"Take care!\"]\n",
        "        },\n",
        "        {\n",
        "            \"tag\": \"thanks\",\n",
        "            \"patterns\": [\"Thanks\", \"Thank you\", \"Thank you so much\"],\n",
        "            \"responses\": [\"You're welcome!\", \"Anytime!\", \"Glad to help!\"]\n",
        "        },\n",
        "        {\n",
        "            \"tag\": \"about\",\n",
        "            \"patterns\": [\"What is your name?\", \"Who are you?\", \"Tell me about yourself\"],\n",
        "            \"responses\": [\"I am a chatbot created to help you!\", \"I am an AI assistant created to chat with you.\"]\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "\n",
        "def preprocess_data(intents):\n",
        "    training_sentences = []\n",
        "    training_labels = []\n",
        "    labels = []\n",
        "\n",
        "    for intent in intents['intents']:\n",
        "        for pattern in intent['patterns']:\n",
        "            word_list = nltk.word_tokenize(pattern)\n",
        "            training_sentences.append(\" \".join([lemmatizer.lemmatize(w.lower()) for w in word_list]))\n",
        "            training_labels.append(intent['tag'])\n",
        "        if intent['tag'] not in labels:\n",
        "            labels.append(intent['tag'])\n",
        "\n",
        "    return training_sentences, training_labels, labels\n",
        "\n",
        "\n",
        "training_sentences, training_labels, labels = preprocess_data(intents)\n",
        "\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "model = make_pipeline(vectorizer, MultinomialNB())\n",
        "model.fit(training_sentences, training_labels)\n",
        "\n",
        "\n",
        "def classify_message(message):\n",
        "    return model.predict([message])[0]\n",
        "def get_response(tag):\n",
        "    for intent in intents['intents']:\n",
        "        if intent['tag'] == tag:\n",
        "            return random.choice(intent['responses'])\n",
        "\n",
        "\n",
        "def chatbot():\n",
        "    print(\"Chatbot: Hello! How can I assist you today?\")\n",
        "    while True:\n",
        "        message = input(\"You: \")\n",
        "\n",
        "        if message.lower() == \"exit\":\n",
        "            print(\"Chatbot: Goodbye!\")\n",
        "            break\n",
        "\n",
        "\n",
        "        tag = classify_message(message.lower())\n",
        "\n",
        "\n",
        "        response = get_response(tag)\n",
        "        print(\"Chatbot:\", response)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    chatbot()\n"
      ]
    }
  ]
}